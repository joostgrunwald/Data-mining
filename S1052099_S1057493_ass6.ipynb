{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Martijn van de Wouw\"\n",
    "STUDENT_NUMBER = \"s1052099\"\n",
    "COLLABORATOR_NAME = \"Joost Grunwald\"\n",
    "COLLABORATOR_STUDENT_NUMBER = \"s1057493\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Important:** When handing in your homework:\n",
    "+ Hand in the notebook (and nothing else) **named as follows**: StudentName1_snumber_StudentName2_snumber.ipynb\n",
    "+ Provide clear and complete answers to the questions below under a separate header (not hidden somewhere in your source code), and make sure to explain your answers / motivate your choices. Add Markdown cells where necessary.\n",
    "+ Source code, output graphs, derivations, etc., should be included in the notebook.\n",
    "+ Hand-in: upload to Brightspace.\n",
    "+ Include name, student number, assignment (especially in filenames)!\n",
    "+ When working in pairs only one of you should upload the assignment, and report the name of your partner in your filename.\n",
    "+ Use the Brightspace discussion board or email the student assistants for questions on how to complete the exercises.\n",
    "+ If you find mistakes/have suggestions/would like to complain about the assigment material itself, please email me [Roel] at `Roel.Bouman@ru.nl`\n",
    "+ Do not add/remove any cells in the notebook, else this might break the auto-grading system. **An invalid notebook will mean a severe reduction in your grade!**\n",
    "+ Only type your answers in those places where they are asked.\n",
    "+ Remove any \"raise NotImplementedError()\" statements in the cells you answered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a0e6a4250cc41f559ef20a6dad74f71",
     "grade": false,
     "grade_id": "cell-4543b21b34b8c4a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 6: Nearest Neighbour and Artificial Neural Networks\n",
    "\n",
    "## Objective of this assignment\n",
    "The objective of this exercise is to understand how k-nearest neighbor and neural networks can be used to solve classification problems.\n",
    "\n",
    "\n",
    "## Advised Reading and Exercise Material\n",
    "**The following reading material is recommended:**\n",
    "\n",
    "- Pang-Ning Tan, Michael Steinbach, and Vipin Kumar, *Introduction to Data Mining*, section 5.2-5.4.\n",
    "\n",
    "\n",
    "## Additional Tools\n",
    "This exercise is based upon material kindly provided by the Cognitive System Section, DTU Compute, http://cogsys.compute.dtu.dk. Any sale or commercial distribution is strictly forbidden.\n",
    "\n",
    "\n",
    "##  6.1 K-nearest neighbor classification\n",
    "In this exercise we will use the k-nearest neighbors (KNN) method for classification.\n",
    "First, we will consider the four synthetic data sets synth1, synth2, synth3 and\n",
    "synth4 we used in earlier assignments.\n",
    "\n",
    "#### 6.1.1 (2 points)\n",
    "For each of the four synthetic data sets, do the following.:\n",
    "\n",
    "* Load the training part of the dataset `X_train` and `y_train` as well as `X_test` and `y_test`.\n",
    "\n",
    "* Fit a  k-nearest neighbor classifier model (`KNeighborsClassifier` from `sklearn.neighbors` (http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)) on the train data. \n",
    "  * Choose a suitable distance measure (you should consider the distance measures `euclidean`,`cityblock`, `cosine`, and `seuclidean`).\n",
    "    * For the `seuclidean` distance you need to supply an additional parameter `V`. `V` should be a **vector** containing the variance of each variable in `X`.\n",
    "\n",
    "  * Choose a suitable number of neighbors. \n",
    "  \n",
    "* Predict the class of the test data using the trained model.\n",
    "\n",
    "* Make a scatterplot of the train and test data with the classification of the test data obtained with the best k-value and distance measures you found -- just one plot per data set is fine. You can use the `classification_plot` function from the toolbox. \n",
    "  * Use the obtained prediction of the test data in your plot.\n",
    "  * Try to study the plot (use all 5 arguments) to see how test data is classified using the train data.\n",
    "\n",
    "* Create the confusion matrix, plot it using the `plot_confusion_matrix` function given below, and calculate the accuracy and error rate. Print or show the accuracy and error rate for each dataset.\n",
    "\n",
    "Answers the following questions for each dataset:\n",
    "\n",
    "* Which distance measures worked best for each of the four problems? Can you explain why?\n",
    "\n",
    "* How many neighbors were needed for the four problems? \n",
    "\n",
    "* Can you give an example of when it would be good to use a large/small number of neighbors? Consider e.g. when clusters are  well separated versus when they are overlapping.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* To generate a confusion matrix, you can use the function confusion_matrix() from the module sklearn.metrics. You can use the function below to plot the confusion matrix. Don't remember how to read a confusion matrix? Check the wiki page: https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cedae35cd068fa593fe2e56341aa9a48",
     "grade": false,
     "grade_id": "cell-881a8d7329bd61d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm,y):\n",
    "    df_cm = pd.DataFrame(cm, index = [i+1 for i in np.unique(y)],\n",
    "                  columns = [i+1 for i in np.unique(y)])\n",
    "    plt.figure()\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.ylabel('Actual class')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6275cd2d972791616a4c4b71c3ec716",
     "grade": true,
     "grade_id": "cell-c914718fc664cbf0",
     "locked": false,
     "points": 150,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'toolbox'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17404/1370228640.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDistanceMetric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_plot\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_plot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'toolbox'"
     ]
    }
   ],
   "source": [
    "import scipy.io as sc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from toolbox.classification_plot import classification_plot as cp\n",
    "\n",
    "\n",
    "def run(file, n, metric, show):\n",
    "    #getting data\n",
    "    data = sc.loadmat(\"./data/\"+file)\n",
    "    X_train = data[\"X_train\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_train = data[\"y_train\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    #needs extra paramater\n",
    "    if (metric == 'seuclidean'):\n",
    "        V = np.var(X_train, axis=0)#calculate extra parameter\n",
    "        KNN = KNeighborsClassifier(n_neighbors=n, algorithm='auto', metric=metric, metric_params={'V':V})\n",
    "    else:\n",
    "        KNN = KNeighborsClassifier(n_neighbors=n, algorithm='auto', metric=metric)\n",
    "    #train\n",
    "    KNN.fit(X_train, y_train.ravel())\n",
    "\n",
    "    #predict with test set\n",
    "    predict = KNN.predict(X_test)\n",
    "    \n",
    "    #make confusion matrix\n",
    "    cm = confusion_matrix(y_test, predict)\n",
    "    t = 0\n",
    "    #get amount of correct predictions\n",
    "    for i in range(len(cm)):\n",
    "        t += cm[i][i]\n",
    "    if(show):\n",
    "        plot = cp(X_test, y_test.ravel(), predict, X_train, y_train.ravel())\n",
    "        plot_confusion_matrix(cm, y_test)\n",
    "    #return values for other functions to use\n",
    "    return t,len(y_test.ravel())\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def getBestK(file):\n",
    "    metrics = ['euclidean', 'cosine', 'manhattan', 'seuclidean']\n",
    "    #best amount of correct predictions per metric\n",
    "    best = 0\n",
    "    #best amont of neighbors per metric\n",
    "    ind = 0\n",
    "    #size of test data\n",
    "    size = 0\n",
    "    #best of all metrics\n",
    "    bestM = 0\n",
    "    #the best metric\n",
    "    M = \"\"\n",
    "    #number of correct predictions of the best metric\n",
    "    indM = 0\n",
    "    \n",
    "    \n",
    "    for m in metrics:\n",
    "        #loop over n neighbors to see amount which does best\n",
    "        for i in range(1, 40):\n",
    "            t,size = run(file, i, m, False)\n",
    "            if (t>best):\n",
    "                best = t\n",
    "                ind = i\n",
    "        if(best > bestM):\n",
    "            bestM = best\n",
    "            M = m\n",
    "            indM = ind\n",
    "        best = 0\n",
    "        ind = 0\n",
    "    #print the results of the best metric\n",
    "    print(\"Distance metric: \\t\" +M)\n",
    "    print(\"Correct predictions: \\t\" +str(bestM))\n",
    "    print(\"\\tAccuracy \\t= \" + str(bestM/size))\n",
    "    print(\"\\tError rate \\t= \" + str((size - bestM) / size))\n",
    "    print(\"With k neighbors: \\t\" +str(indM))\n",
    "    #show the plot and confusion matrix of best metric\n",
    "    run(file, indM, M, True)\n",
    "    \n",
    "def runfiles():\n",
    "    #go over all the files\n",
    "    files = [\"synth1.mat\", \"synth2.mat\",\"synth3.mat\",\"synth4.mat\"]\n",
    "    for i in files:\n",
    "        print(\"CALCULATING FOR FILE \\t\" + i)\n",
    "        print(\"=======================\")\n",
    "        getBestK(i)\n",
    "        print(\"=======================\")\n",
    "        \n",
    "runfiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81aaceddeadd958936871915d9c9644d",
     "grade": true,
     "grade_id": "cell-c365d26e6855010f",
     "locked": false,
     "points": 50,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# GIVE ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "643fb729244ae2edc7a62e71f90f5dd7",
     "grade": false,
     "grade_id": "cell-8ecc7df13886d8fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 6.1.2 (1.25 points)\n",
    "In general we can use cross-validation to select the best parameters for our method.This can however be computationally expensive. \n",
    "\n",
    "We now return to the Iris data that we have considered in previous exercises, and will attempt to classify the Iris flowers using KNN. \n",
    "\n",
    "* Load the Iris data into Python with the `pandas` function `read_excel()` and save it to a variable called `iris_data`. Inspect the data by printing the `head()` of the dataframe. \n",
    "\n",
    "* Use the values of the 4 variables `Sepal Length,  Sepal Width,  Petal Length,  Petal Width` to create a data set `X`. Use the `Type` column to create the labels `labels`. Both `X` should be a numpy array! You can use the `to_numpy()` method for this. `labels` will be a Pandas series.\n",
    "\n",
    "* Convert the Pandas series `labels` to an integer encoding by using the `LabelEncoder` in scikit-learn. Save the integer encoding, which should be a numpy array to a variable called `y`.\n",
    "\n",
    "* Use leave-one-out cross-validation to estimate the optimal number of neighbors, k, for the k-nearest neighbor classifier. Save the vector of averaged errors to a variable called `mean_errors`. This should be a numpy array.\n",
    "\n",
    "* Plot the cross-validation average classification error as a function of k for $k = 1,..,40$.\n",
    "\n",
    "You can use the function `LeaveOneOut` from `sklearn.model_selection`: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html\n",
    "\n",
    "What is the optimal number of neighbors to use for this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bce13c66bbd7e3011f794f5ec64f5b4",
     "grade": false,
     "grade_id": "cell-d4a09338afe0c7e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import matplotlib.pyplot as plt\n",
    "iris_data = pd.read_excel(\"data\\iris.xls\")\n",
    "print(iris_data.head())\n",
    "#prepare data\n",
    "X = iris_data.iloc[:,0:3].to_numpy()\n",
    "labels = iris_data.iloc[:,-1]\n",
    "y = preprocessing.LabelEncoder().fit(labels).transform(labels)\n",
    "#create splits\n",
    "split = LeaveOneOut()\n",
    "split.get_n_splits(X)\n",
    "mean_errors = []\n",
    "ns = list(range(1,150))\n",
    "for i in range(40):\n",
    "    err = []\n",
    "    for train_index, test_index in split.split(X):\n",
    "        #create data splits\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "        KNN = KNeighborsClassifier(n_neighbors = ns[i], algorithm='auto', metric = 'euclidean')\n",
    "        #train\n",
    "        KNN.fit(X_train, y_train)\n",
    "\n",
    "        #predict with test set\n",
    "        predict = KNN.predict(X_test)\n",
    "        if(predict[0] == y_test[0]):\n",
    "            err.append(0)\n",
    "        else:\n",
    "            err.append(1)\n",
    "    errnp = np.asarray(err)\n",
    "\n",
    "    mean_errors.append(errnp.mean())\n",
    "#plot figure\n",
    "plt.figure()\n",
    "plt.plot(list(range(1,41)), np.asarray(mean_errors))\n",
    "plt.xlabel('n neighbors')\n",
    "plt.ylabel('averaged error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99bc4874acec02527a2cc4b8100b7848",
     "grade": true,
     "grade_id": "cell-5228b8b55ea59d61",
     "locked": true,
     "points": 100,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Checks whether 6.1.2 output is correct\"\"\"\n",
    "\"\"\"DO NOT MODIFY THIS CELL\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50a9bf11b4aef02a9e0d400dee27e050",
     "grade": true,
     "grade_id": "cell-28bebc3e081a2ae6",
     "locked": false,
     "points": 25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "for this data it seems that around 11 neighbours gives the best result, it seems that when we use less we are more sensitive to outliers and when we use more we take too many data points into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95d4dc0b1fb23be2109840ed43fb228e",
     "grade": false,
     "grade_id": "cell-d5c684910893698d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 6.2 Artificial Neural Networks\n",
    "\n",
    "In this part of the exercise we will use neural networks to classify the xor data. We will consider a network with an input layer, one layer of hidden units and an output layer. The class `sklearn.neural_network.MLPClassifier` can be used to create a Multilayer Perceptron that can minimizes the Cross-Entropy loss function for any dataset X with corresponding labels y. See https://scikit-learn.org/stable/modules/neural_networks_supervised.html \n",
    "\n",
    "We use the data contained in `xor.mat` in the Data folder. \n",
    "\n",
    "#### 6.2.1 (0.5 points)\n",
    "Check out the documentation for `MLPClassifier` and read the documentation well. Make sure you understand at least in general terms how the learning process works. Answer the following questions before you continue:\n",
    "\n",
    "1. For a single perceptron, the activation function can be linear, e.g. $f(x) = x$. However, the activation function used in the MLP class is a non-linear function. Why does it not make sense for a MLP to use such a linear activation function? Hint: what does the following computation simplify to if $f(x)$ is such a simple linear function: $f(Wo*f(Wh*X_i))$?\n",
    "2. The MLPClassifier has a few optional parameters. For each of the following parameters, explain how changing the parameter might affect the learning process or the resulting solution:\n",
    "\n",
    "> `hidden_layer_sizes`:\n",
    "\n",
    "> `max_iter`:\n",
    "\n",
    "> `learning_rate`:\n",
    "\n",
    "> `learning_rate_init`:\n",
    "\n",
    "   3\\. Use the following commands to create a small test set:\n",
    "\n",
    "> `X = np.array([[0,0],[0,1],[1,0],[1,1]]).`\n",
    "\n",
    "> `y = np.array([0,0,0,1])`\n",
    "     \n",
    "\n",
    "Then: \n",
    "- Create a MLPClassifier with 1 hidden layer containing 1 unit using the `lbfgs`solver and fit the data. You can leave the other parameters unchanged. Use the `score` method of the classifier object to compute the mean accuracy. How well does the MLP perform on this problem? Use the function `plot_boundaries` function from the toolbox to plot the data and the decision boundaries. Why does(n't) it work well with one hidden unit? Could you improve by using more?\n",
    "\n",
    "#### NB: the weights are initialised randomly, so you should run the code a few times untill you get a decent result.\n",
    "\n",
    "**Helpful hint:**\n",
    "- You can plot decision boundaries as follows:\n",
    "\n",
    "`plot_boundaries(X,y,clf)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d2db60b6ac94b26be0ce4f22c3792ef",
     "grade": true,
     "grade_id": "cell-9d41b67efb609fc1",
     "locked": false,
     "points": 25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73bfe7e2649db80247ed098f87b38a1e",
     "grade": true,
     "grade_id": "cell-735ec50c840768e5",
     "locked": false,
     "points": 25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##Answer to question 6.2.1\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d4d46dfc897998e2d3a0f612227cdfe",
     "grade": false,
     "grade_id": "cell-c65eb2c09f090f0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 6.2.2 loading and visualizing the Xor data (0.5)\n",
    "In this exercise, we'll look at the Xor data in `xor.mat`.\n",
    "\n",
    "\n",
    "- Load the data into Python using `scipio.io.loadmat(...)` and make `y` a 1d vector. Make a scatter plot of the two attributes in `X`, coloring the points according to the class labels `y` and add a legend. Use matplotlib functions, and no toolbox functions for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "726ccc0e9c3bd157fbc642f853a19dad",
     "grade": true,
     "grade_id": "cell-29ae1f3886162c7e",
     "locked": false,
     "points": 50,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##Answer to question 6.2.2\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e69b42d6ee8a293a44e0a254ea4095ac",
     "grade": false,
     "grade_id": "cell-651d2f68691aeb95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 6.2.3 Optimizing the number of hidden units (3 points) \n",
    "In this exercise, we'll start optimizing a MLPClassifier for use on the Xor data. We've toyed around a bit with finding the optimal number of hidden units, but now we'll optimize them in earnest.\n",
    "\n",
    "- Create a CV loop for MLPClassifier with 1 to 20 hidden units. Fit X and y. Use 10-fold cross-validation `KFold` from `sklearn.model_selection` to create training and test sets and estimate the classification error for both. **In each fold, for each numberof hidden units, run the learning process 5 times and take the average classification error.**  \n",
    "- Determine the optimal number of hidden units.\n",
    "- Plot the decision boundaries of one network with the optimal number of hidden units trained on the entire data set, and explain why the network performs so well/poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b57960909eb8da56a2d80344b1570c9",
     "grade": true,
     "grade_id": "cell-43a21e78b44b6141",
     "locked": false,
     "points": 250,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##Answer to question 6.2.3\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec4e9ba786b4d7af64e24f6158c4ad16",
     "grade": true,
     "grade_id": "cell-adb15a9e87187331",
     "locked": false,
     "points": 50,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27cfad07a10a272fc8a009aa2ebd1b86",
     "grade": false,
     "grade_id": "cell-e96de244ac8aeee8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 6.3 Support Vector Machines\n",
    "\n",
    "In this subsection we're taking a look at Support Vector Machines.\n",
    "We'll again analyze the Xor data. and look at what some of the hyperparameters do.\n",
    "\n",
    "### 6.3.1 Linear kernels (1 point)\n",
    "- First, use a linear kernel support vector machine from Scikit-learn, `SVC`, and train it on the Xor data. \n",
    "- Again, plot the boundaries of the classification using the `plot_boundaries` function from the toolbox\n",
    "- What do you observe when you use the basic settings, but with a linear kernel? Why does or doesn't the model perform well? \n",
    "- Do you get different results when rerunning the algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce21ace93dd0a8b99404ad22976840a5",
     "grade": true,
     "grade_id": "cell-db326907accda924",
     "locked": false,
     "points": 50,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from toolbox.plot_boundaries import plot_boundaries as pb\n",
    "data = sc.loadmat(\"./data/xor.mat\")\n",
    "X = data['X']\n",
    "y = data['y'].ravel()\n",
    "svc = SVC(kernel = 'linear')\n",
    "svc.fit(X, y)\n",
    "pb(X, y, svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a650edbdd5c5effc776270b529e0f0f",
     "grade": true,
     "grade_id": "cell-27dfca3ea13a3b24",
     "locked": false,
     "points": 25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "because of the linear model, svc is not able to make correct predictions for each cluster and thus has to 'choose' which cluster to predict wrongly. When rerunning the results stay the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf0ce64bd32ece663e556521d0200be6",
     "grade": false,
     "grade_id": "cell-e4157c8d905eeb8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 6.3.2 The kernel trick (1.25 points)\n",
    "In order to solve the problem a linear kernel SVM has when facing such multimodal data, we can transform the data using a so called kernel. Several kernels exist, but in this case we'll look into a second degree polynomial kernel first.\n",
    "\n",
    "There are several ways to construct a second degree polynomial kernel\n",
    "- You can include or exclude the bias (the ^0 term)\n",
    "- You can include or exclude non-interactions (X_1^d) terms\n",
    "\n",
    "In this case, we can just add a third variable/feature to the data, namely the interaction term $x_1 * x_2$. So we don't include the $x_1^2$ and $x_2^2$ terms.\n",
    "\n",
    "- Calculate the third interaction feature for the kernel. You can do this manually, or using the `PolynomialFeatures` function from Scikit-learn with the correct settings. Verify you end up with a $400*3$ matrix.\n",
    "- Make a 3D scatterplot of the transformed data, color the points by the label and add a legend\n",
    "- Use a linear kernel SVM with otherwise default settings to predict `y`. What is the accuracy of the classifier now? Does this match up with what you see in the plot? Can you find a separating hyperplane?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "506b2ebbb3926c9aa6fbf987495a1623",
     "grade": true,
     "grade_id": "cell-46b977b06d43a181",
     "locked": false,
     "points": 100,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree = 2, interaction_only=True)\n",
    "data = poly.fit_transform(X)[:,1:4]\n",
    "print(data.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(data[(y==1),0],data[(y==1),1],data[(y==1),2],label='Label = 1', c=\"r\", marker=\"o\")\n",
    "ax.scatter(data[(y==0),0],data[(y==0),1],data[(y==0),2],label='Label = 0', c=\"b\", marker=\"o\")\n",
    "ax.legend()\n",
    "svc.fit(data, y)\n",
    "predict = svc.predict(data)\n",
    "print(y[predict==y])\n",
    "print(len(y))\n",
    "print(len(y[predict==y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e39579e2161b5cc5d0398c59965f0872",
     "grade": true,
     "grade_id": "cell-ef08773205385f43",
     "locked": false,
     "points": 25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "accuracy is 398/400 which is a bit less than 1000 %, I think it does match up as I do see a 2d plane (hyperplane) which can correctly separate the points. it would be a curved hyperplane which bends as a sort of dome over the points with label 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c716cfc2f4953ba0b22d73843b5c94a",
     "grade": false,
     "grade_id": "cell-bdfbc58160e0ae4f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 6.3.3 Using the kernel trick directly (0.75 points)\n",
    "\n",
    "Instead of manually applying the kernel trick, we can also use the polynomial kernel directly when initializing the SVC object. \n",
    "\n",
    "- Apply SVM using a second degree polynomial kernel and calculate and print the score.\n",
    "- Use the `plot_boundaries` function to show the classification boundaries. \n",
    "- Is the result the same as when applying the trick manually? Why (not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23167b6d100d841b1d65721f427472ab",
     "grade": true,
     "grade_id": "cell-5610b69451dc2c45",
     "locked": false,
     "points": 50,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "svc = SVC(degree = 2,kernel = 'poly')\n",
    "svc.fit(data, y)\n",
    "svc.predict(data)\n",
    "predict = svc.predict(data)\n",
    "score = svc.score(data, y)\n",
    "#pb(data, y, svc)\n",
    "#does not seem to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d82556b7e8a181c99f91e0d033e605d4",
     "grade": true,
     "grade_id": "cell-67b6991153e5b137",
     "locked": false,
     "points": 25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "it is, it probably has to do with the extra dimensenion the linear SVM gets which makes it have the same result as the polynomial one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
